{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRxMZ6TNHdVj",
        "outputId": "e306f127-2a7f-405d-d61c-3e907d00dcaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.11.17)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=ecfc3e6d26ce918ad216669d411e35002922c32e8e05dfa3909082a23e303230\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Collecting microsofttranslator\n",
            "  Downloading microsofttranslator-0.8.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from microsofttranslator) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from microsofttranslator) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=1.2.3->microsofttranslator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=1.2.3->microsofttranslator) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=1.2.3->microsofttranslator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=1.2.3->microsofttranslator) (2023.11.17)\n",
            "Building wheels for collected packages: microsofttranslator\n",
            "  Building wheel for microsofttranslator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for microsofttranslator: filename=microsofttranslator-0.8-py3-none-any.whl size=6988 sha256=927c36318c36939d7d1b7fa19d0cd9ff09721fb60ea0c2659b58021e7ff6ff2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f9/8c/ab0a64d1d08e32a57ce383a68bf706882738c76a16559b1f63\n",
            "Successfully built microsofttranslator\n",
            "Installing collected packages: microsofttranslator\n",
            "Successfully installed microsofttranslator-0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install microsofttranslator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM1kDEqNmV21",
        "outputId": "f034ecd9-8a18-46a8-b215-211a29fad677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Date       Time                                           Comments  \\\n",
            "0 2023-09-01  15:55:08Z  LG TV's are the biggest piece of junk I've eve...   \n",
            "1 2023-09-01  14:06:05Z                          Sent from my Huawei phone   \n",
            "2 2023-09-01  23:37:30Z  Lived in ksa my entire childhood currently 20 ...   \n",
            "3 2023-09-02  11:45:28Z  You just need to buy # one general#. Any how, ...   \n",
            "4 2023-09-02  02:26:34Z  You all people are right due to technology boo...   \n",
            "\n",
            "                                 Translated_Comments  \\\n",
            "0  lg tvs are the biggest piece of junk ive ever ...   \n",
            "1                          sent from my huawei phone   \n",
            "2  lived in ksa my entire childhood currently  mo...   \n",
            "3  you just need to buy  one general any how disa...   \n",
            "4  you all people are right due to technology boo...   \n",
            "\n",
            "                                            POS_Tags  Senti_Score  \\\n",
            "0  [(lg, NN), (tvs, NNS), (are, VBP), (the, DT), ...        0.500   \n",
            "1  [(sent, NN), (from, IN), (my, PRP$), (huawei, ...        0.000   \n",
            "2  [(lived, VBN), (in, IN), (ksa, NN), (my, PRP$)...        0.000   \n",
            "3  [(you, PRP), (just, RB), (need, VB), (to, TO),...        0.000   \n",
            "4  [(you, PRP), (all, DT), (people, NNS), (are, V...       -0.875   \n",
            "\n",
            "              Labels  \n",
            "0  strongly positive  \n",
            "1            neutral  \n",
            "2            neutral  \n",
            "3            neutral  \n",
            "4  strongly negative  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from googletrans import Translator\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def calculate_tweet_sentiment(tweet_content):\n",
        "    pos_score = 0\n",
        "    neg_score = 0\n",
        "\n",
        "    words = word_tokenize(tweet_content)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for word, pos in pos_tag(words):\n",
        "        wordnet_tag = get_wordnet_pos(pos)\n",
        "        wordnet_synsets = list(swn.senti_synsets(lemmatizer.lemmatize(word), wordnet_tag))\n",
        "        if wordnet_synsets:\n",
        "            synset = wordnet_synsets[0]\n",
        "            pos_score += synset.pos_score()\n",
        "            neg_score += synset.neg_score()\n",
        "\n",
        "    sentiment_score = pos_score - neg_score\n",
        "    return sentiment_score  # Returning the sentiment score itself\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.xlsx' with your file path)\n",
        "file_path = '/content/September 2023 PB.xlsx'  # Replace this with your file path\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "# Preprocessing and translation of the 'Comments' column\n",
        "translator = Translator()\n",
        "\n",
        "def preprocess_translate(comment):\n",
        "    comment = comment.lower()\n",
        "    comment = ''.join([c for c in comment if c.isalpha() or c.isspace()])\n",
        "    if comment.strip().lower() == comment:  # Check if the comment is in English\n",
        "        return comment  # Skip translation for English comments\n",
        "    try:\n",
        "        translated = translator.translate(comment, src='auto', dest='en').text\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        return comment  # Return the original text if translation fails\n",
        "\n",
        "# Apply preprocessing and translation to 'Comments' column\n",
        "df['Translated_Comments'] = df['Comments'].apply(preprocess_translate)\n",
        "\n",
        "# Extract POS tags for each comment\n",
        "def get_pos_tags(comment):\n",
        "    words = word_tokenize(comment)\n",
        "    return pos_tag(words)\n",
        "\n",
        "df['POS_Tags'] = df['Translated_Comments'].apply(get_pos_tags)\n",
        "# Add a new column 'POS_Tags' containing the POS tags for each comment\n",
        "\n",
        "# Apply sentiment analysis to the translated comments\n",
        "df['Senti_Score'] = df['Translated_Comments'].apply(calculate_tweet_sentiment)\n",
        "\n",
        "# Apply labels based on sentiment score using SentiWordNet scores\n",
        "def get_sentiment_label(score):\n",
        "    pos_threshold = 0.25\n",
        "    neg_threshold = -0.25\n",
        "\n",
        "    if score > pos_threshold:\n",
        "        return 'strongly positive'\n",
        "    elif score > 0 and score <= pos_threshold:\n",
        "        return 'weakly positive'\n",
        "    elif score < neg_threshold:\n",
        "        return 'strongly negative'\n",
        "    elif score >= 0 and score < neg_threshold:\n",
        "        return 'weakly negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['Labels'] = df['Senti_Score'].apply(get_sentiment_label)\n",
        "# Add a new column 'Labels' containing the sentiment labels based on scores\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Extracting filename from the file path\n",
        "file_name = file_path.split('/')[-1]  # Extracting only the filename from the path\n",
        "\n",
        "# Add 'Updated_' prefix to the filename\n",
        "updated_file_name = '/content/Updated_' + file_name\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel(updated_file_name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz9gxhupWlaQ"
      },
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwTGQ397WPWx"
      },
      "source": [
        "#Use This Section if you face any error in the translating the Comments column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa8bo__bHP3d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from googletrans import Translator\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def calculate_tweet_sentiment(tweet_content):\n",
        "    pos_score = 0\n",
        "    neg_score = 0\n",
        "\n",
        "    words = word_tokenize(tweet_content)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for word, pos in pos_tag(words):\n",
        "        wordnet_tag = get_wordnet_pos(pos)\n",
        "        wordnet_synsets = list(swn.senti_synsets(lemmatizer.lemmatize(word), wordnet_tag))\n",
        "        if wordnet_synsets:\n",
        "            synset = wordnet_synsets[0]\n",
        "            pos_score += synset.pos_score()\n",
        "            neg_score += synset.neg_score()\n",
        "\n",
        "    sentiment_score = pos_score - neg_score\n",
        "    return sentiment_score  # Returning the sentiment score itself\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.xlsx' with your file path)\n",
        "file_path = '/content/Jan 2023 PF.xlsx'  # Replace this with your file path\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "# Preprocessing and translation of the 'Comments' column\n",
        "translator = Translator()\n",
        "\n",
        "def preprocess_translate(comment):\n",
        "    if isinstance(comment, str):  # Check if the comment is a string\n",
        "        comment = comment.lower()\n",
        "        comment = ''.join([c for c in comment if c.isalpha() or c.isspace()])\n",
        "        if comment.strip().lower() == comment:  # Check if the comment is in English\n",
        "            return comment  # Skip translation for English comments\n",
        "        try:\n",
        "            translated = translator.translate(comment, src='ur', dest='en').text  # Translate from Urdu to English\n",
        "            return translated\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return comment  # Return the original text if translation fails\n",
        "    else:\n",
        "        return str(comment)  # Convert non-string data to string\n",
        "\n",
        "\n",
        "# Apply preprocessing and translation to 'Comments' column\n",
        "df['Translated_Comments'] = df['Comments'].apply(preprocess_translate)\n",
        "\n",
        "# Extract POS tags for each comment\n",
        "def get_pos_tags(comment):\n",
        "    words = word_tokenize(comment)\n",
        "    return pos_tag(words)\n",
        "\n",
        "df['POS_Tags'] = df['Translated_Comments'].apply(get_pos_tags)\n",
        "# Add a new column 'POS_Tags' containing the POS tags for each comment\n",
        "\n",
        "# Apply sentiment analysis to the translated comments\n",
        "df['Senti_Score'] = df['Translated_Comments'].apply(calculate_tweet_sentiment)\n",
        "\n",
        "# Apply labels based on sentiment score using SentiWordNet scores\n",
        "def get_sentiment_label(score):\n",
        "    pos_threshold = 0.25\n",
        "    neg_threshold = -0.25\n",
        "\n",
        "    if score > pos_threshold:\n",
        "        return 'strongly positive'\n",
        "    elif score > 0 and score <= pos_threshold:\n",
        "        return 'weakly positive'\n",
        "    elif score < neg_threshold:\n",
        "        return 'strongly negative'\n",
        "    elif score >= 0 and score < neg_threshold:\n",
        "        return 'weakly negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['Labels'] = df['Senti_Score'].apply(get_sentiment_label)\n",
        "# Add a new column 'Labels' containing the sentiment labels based on scores\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Extracting filename from the file path\n",
        "file_name = file_path.split('/')[-1]  # Extracting only the filename from the path\n",
        "\n",
        "# Add 'Updated_' prefix to the filename\n",
        "updated_file_name = '/content/Updated_' + file_name\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel(updated_file_name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViFrbbzxS6tU"
      },
      "source": [
        "#Microsoft Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmVoqCehUCRl",
        "outputId": "b4c3b312-7b78-4871-880e-0aedc717d07b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation error: Translator.translate() got an unexpected keyword argument 'lang_from'\n",
            "Translation error: Translator.translate() got an unexpected keyword argument 'lang_from'\n",
            "Translation error: Translator.translate() got an unexpected keyword argument 'lang_from'\n",
            "Translation error: Translator.translate() got an unexpected keyword argument 'lang_from'\n",
            "Translation error: Translator.translate() got an unexpected keyword argument 'lang_from'\n",
            "        Date           Time  \\\n",
            "0 2023-09-10  12:21:57+0000   \n",
            "1 2023-09-10  07:43:42+0000   \n",
            "2 2023-09-11  11:23:00+0000   \n",
            "3 2023-09-12  11:57:34+0000   \n",
            "4 2023-09-13  11:10:55+0000   \n",
            "\n",
            "                                            Comments  \\\n",
            "0  ال کلاس فور ملازمينو حکومت نه غوښته کړي چي دوي...   \n",
            "1  پنجاب میں سارے لوگ ڈیوٹی پر ہے کام کر رہا ہے ا...   \n",
            "2  پاکستان کی ترقی کے لیے موجودہ صوبوں کی تقسیم  ...   \n",
            "3  پشاور کالج کے اساتذہ کا پروموشن بورڈ کے خلاف ا...   \n",
            "4  پی ڈی اے ایمپلائز اینڈ ورکرز یونین کا مطالبات ...   \n",
            "\n",
            "                                 Translated_Comments  \\\n",
            "0  ال کلاس فور ملازمينو حکومت نه غوښته کړي چي دوي...   \n",
            "1  پنجاب میں سارے لوگ ڈیوٹی پر ہے کام کر رہا ہے ا...   \n",
            "2  پاکستان کی ترقی کے لیے موجودہ صوبوں کی تقسیم  ...   \n",
            "3  پشاور کالج کے اساتذہ کا پروموشن بورڈ کے خلاف ا...   \n",
            "4  پی ڈی اے ایمپلائز اینڈ ورکرز یونین کا مطالبات ...   \n",
            "\n",
            "                                            POS_Tags  Senti_Score  \\\n",
            "0  [(ال, JJ), (کلاس, NNP), (فور, NNP), (ملازمينو,...          0.0   \n",
            "1  [(پنجاب, JJ), (میں, NNP), (سارے, NNP), (لوگ, N...          0.0   \n",
            "2  [(پاکستان, JJ), (کی, NNP), (ترقی, NNP), (کے, N...          1.5   \n",
            "3  [(پشاور, JJ), (کالج, NNP), (کے, NNP), (اساتذہ,...          0.0   \n",
            "4  [(پی, JJ), (ڈی, NNP), (اے, NNP), (ایمپلائز, NN...          0.0   \n",
            "\n",
            "              Labels  \n",
            "0            neutral  \n",
            "1            neutral  \n",
            "2  strongly positive  \n",
            "3            neutral  \n",
            "4            neutral  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from microsofttranslator import Translator\n",
        "import nltk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def calculate_tweet_sentiment(tweet_content):\n",
        "    pos_score = 0\n",
        "    neg_score = 0\n",
        "\n",
        "    words = word_tokenize(tweet_content)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for word, pos in pos_tag(words):\n",
        "        wordnet_tag = get_wordnet_pos(pos)\n",
        "        wordnet_synsets = list(swn.senti_synsets(lemmatizer.lemmatize(word), wordnet_tag))\n",
        "        if wordnet_synsets:\n",
        "            synset = wordnet_synsets[0]\n",
        "            pos_score += synset.pos_score()\n",
        "            neg_score += synset.neg_score()\n",
        "\n",
        "    sentiment_score = pos_score - neg_score\n",
        "    return sentiment_score  # Returning the sentiment score itself\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Load your dataset (replace 'your_dataset.xlsx' with your file path)\n",
        "file_path = '/content/September 2023 JB.xlsx'  # Replace this with your file path\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Initialize the Microsoft Translator\n",
        "translator = Translator('YOUR_CLIENT_ID', 'YOUR_CLIENT_SECRET')\n",
        "\n",
        "# Define the translation function\n",
        "def preprocess_translate(comment):\n",
        "    if isinstance(comment, str):  # Check if the comment is a string\n",
        "        comment = comment.lower()\n",
        "        comment = ''.join([c for c in comment if c.isalpha() or c.isspace()])\n",
        "        if comment.strip().lower() == comment:  # Check if the comment is in English\n",
        "            return comment  # Skip translation for English comments\n",
        "        try:\n",
        "            translated = translator.translate(comment, lang_from='auto', lang_to='en')\n",
        "            return translated\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return comment  # Return the original text if translation fails\n",
        "    else:\n",
        "        return str(comment)  # Convert non-string data to string\n",
        "\n",
        "\n",
        "# Apply preprocessing and translation to 'Comments' column\n",
        "df['Translated_Comments'] = df['Comments'].apply(preprocess_translate)\n",
        "\n",
        "# Extract POS tags for each comment\n",
        "def get_pos_tags(comment):\n",
        "    words = word_tokenize(comment)\n",
        "    return pos_tag(words)\n",
        "\n",
        "df['POS_Tags'] = df['Translated_Comments'].apply(get_pos_tags)\n",
        "# Add a new column 'POS_Tags' containing the POS tags for each comment\n",
        "\n",
        "# Apply sentiment analysis to the translated comments\n",
        "df['Senti_Score'] = df['Translated_Comments'].apply(calculate_tweet_sentiment)\n",
        "\n",
        "# Apply labels based on sentiment score using SentiWordNet scores\n",
        "def get_sentiment_label(score):\n",
        "    pos_threshold = 0.25\n",
        "    neg_threshold = -0.25\n",
        "\n",
        "    if score > pos_threshold:\n",
        "        return 'strongly positive'\n",
        "    elif score > 0 and score <= pos_threshold:\n",
        "        return 'weakly positive'\n",
        "    elif score < neg_threshold:\n",
        "        return 'strongly negative'\n",
        "    elif score >= 0 and score < neg_threshold:\n",
        "        return 'weakly negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['Labels'] = df['Senti_Score'].apply(get_sentiment_label)\n",
        "# Add a new column 'Labels' containing the sentiment labels based on scores\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Extracting filename from the file path\n",
        "file_name = file_path.split('/')[-1]  # Extracting only the filename from the path\n",
        "\n",
        "# Add 'Updated_' prefix to the filename\n",
        "updated_file_name = '/content/Updated_' + file_name\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel(updated_file_name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u0RTR_-Gplg"
      },
      "source": [
        "#DeepL Tansalator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR_y7NcxFg9W",
        "outputId": "c6ead772-00b3-4c5a-e09c-d9914cc2af24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation error: 403\n",
            "Translation error: 403\n",
            "Translation error: 403\n",
            "Translation error: 403\n",
            "Translation error: 403\n",
            "        Date           Time  \\\n",
            "0 2023-09-10  12:21:57+0000   \n",
            "1 2023-09-10  07:43:42+0000   \n",
            "2 2023-09-11  11:23:00+0000   \n",
            "3 2023-09-12  11:57:34+0000   \n",
            "4 2023-09-13  11:10:55+0000   \n",
            "\n",
            "                                            Comments  \\\n",
            "0  ال کلاس فور ملازمينو حکومت نه غوښته کړي چي دوي...   \n",
            "1  پنجاب میں سارے لوگ ڈیوٹی پر ہے کام کر رہا ہے ا...   \n",
            "2  پاکستان کی ترقی کے لیے موجودہ صوبوں کی تقسیم  ...   \n",
            "3  پشاور کالج کے اساتذہ کا پروموشن بورڈ کے خلاف ا...   \n",
            "4  پی ڈی اے ایمپلائز اینڈ ورکرز یونین کا مطالبات ...   \n",
            "\n",
            "                                 Translated_Comments  Senti_Score  \\\n",
            "0  ال کلاس فور ملازمينو حکومت نه غوښته کړي چي دوي...          0.0   \n",
            "1  پنجاب میں سارے لوگ ڈیوٹی پر ہے کام کر رہا ہے ا...          0.0   \n",
            "2  پاکستان کی ترقی کے لیے موجودہ صوبوں کی تقسیم  ...          1.5   \n",
            "3  پشاور کالج کے اساتذہ کا پروموشن بورڈ کے خلاف ا...          0.0   \n",
            "4  پی ڈی اے ایمپلائز اینڈ ورکرز یونین کا مطالبات ...          0.0   \n",
            "\n",
            "              Labels  \n",
            "0            neutral  \n",
            "1            neutral  \n",
            "2  strongly positive  \n",
            "3            neutral  \n",
            "4            neutral  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import requests\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Function to perform translation using DeepL API\n",
        "def deepl_translate(text, api_key):\n",
        "    url = \"https://api.deepl.com/v2/translate\"\n",
        "    params = {\n",
        "        'auth_key': api_key,\n",
        "        'text': text,\n",
        "        'target_lang': 'EN'\n",
        "    }\n",
        "    response = requests.post(url, data=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['translations'][0]['text']\n",
        "    else:\n",
        "        print(f\"Translation error: {response.status_code}\")\n",
        "        return text\n",
        "\n",
        "# Function to preprocess and translate comments\n",
        "def preprocess_translate(comment):\n",
        "    if isinstance(comment, str):\n",
        "        comment = comment.lower()\n",
        "        comment = ''.join([c for c in comment if c.isalpha() or c.isspace()])\n",
        "        if comment.strip().lower() == comment:\n",
        "            return comment\n",
        "        try:\n",
        "            translated = deepl_translate(comment, 'YOUR_API_KEY')\n",
        "            return translated\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return comment\n",
        "    else:\n",
        "        return str(comment)\n",
        "\n",
        "# Function to calculate sentiment score\n",
        "def calculate_tweet_sentiment(tweet_content):\n",
        "    pos_score = 0\n",
        "    neg_score = 0\n",
        "\n",
        "    words = word_tokenize(tweet_content)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for word, pos in pos_tag(words):\n",
        "        wordnet_tag = get_wordnet_pos(pos)\n",
        "        wordnet_synsets = list(swn.senti_synsets(lemmatizer.lemmatize(word), wordnet_tag))\n",
        "        if wordnet_synsets:\n",
        "            synset = wordnet_synsets[0]\n",
        "            pos_score += synset.pos_score()\n",
        "            neg_score += synset.neg_score()\n",
        "\n",
        "    sentiment_score = pos_score - neg_score\n",
        "    return sentiment_score\n",
        "\n",
        "# Function to get WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/September 2023 JB.xlsx'  # Replace with your file path\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Apply preprocessing and translation to 'Comments' column\n",
        "df['Translated_Comments'] = df['Comments'].apply(preprocess_translate)\n",
        "\n",
        "# Apply sentiment analysis to the translated comments\n",
        "df['Senti_Score'] = df['Translated_Comments'].apply(calculate_tweet_sentiment)\n",
        "\n",
        "# Apply labels based on sentiment score using SentiWordNet scores\n",
        "def get_sentiment_label(score):\n",
        "    pos_threshold = 0.25\n",
        "    neg_threshold = -0.25\n",
        "\n",
        "    if score > pos_threshold:\n",
        "        return 'strongly positive'\n",
        "    elif score > 0 and score <= pos_threshold:\n",
        "        return 'weakly positive'\n",
        "    elif score < neg_threshold:\n",
        "        return 'strongly negative'\n",
        "    elif score >= 0 and score < neg_threshold:\n",
        "        return 'weakly negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['Labels'] = df['Senti_Score'].apply(get_sentiment_label)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Extracting filename from the file path\n",
        "file_name = file_path.split('/')[-1]\n",
        "updated_file_name = '/content/Updated_' + file_name\n",
        "\n",
        "# Save the updated DataFrame to a new Excel file\n",
        "df.to_excel(updated_file_name, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}